{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11317610-7c72-40dd-bc63-d0f7d481be91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ADLS via SAS (FixedSASTokenProvider) — short & robust -----------------\n",
    "storage_account = \"stnzrentdev\"\n",
    "container = \"nz-rent\"\n",
    "dfs_fqdn = f\"{storage_account}.dfs.core.windows.net\"\n",
    "abfss_url = f\"abfss://{container}@{dfs_fqdn}/\"\n",
    "\n",
    "# Use token WITHOUT leading '?'\n",
    "sas_token_raw = \"sv=2024-11-04&ss=bfqt&srt=co&sp=rwdlacupyx&se=2025-10-25T16:26:07Z&st=2025-10-15T08:11:07Z&spr=https&sig=Xddwgamve%2Fr6c2FKAWLKWax2cOWBZwUJ5t%2BpmxPWOdg%3D\"\n",
    "\n",
    "# 0) Clear possible conflicting configs (ignore errors if not set)\n",
    "for k in [\n",
    "    f\"fs.azure.account.key.{dfs_fqdn}\",\n",
    "    f\"fs.azure.sas.{container}.{storage_account}.dfs.core.windows.net\",\n",
    "]:\n",
    "    try: spark.conf.unset(k)\n",
    "    except Exception: pass\n",
    "\n",
    "# 1) Tell Spark to use SAS with FixedSASTokenProvider\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{dfs_fqdn}\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{dfs_fqdn}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{dfs_fqdn}\", sas_token_raw)\n",
    "\n",
    "# 2) List container; bootstrap /bronze if empty\n",
    "entries = dbutils.fs.ls(abfss_url)\n",
    "if len(entries) == 0:\n",
    "    dbutils.fs.mkdirs(abfss_url + \"bronze\")\n",
    "    dbutils.fs.put(abfss_url + \"bronze/_sanity.txt\", \"hello databricks\", overwrite=True)\n",
    "    entries = dbutils.fs.ls(abfss_url)\n",
    "\n",
    "# 3) Show as table\n",
    "rows = [(e.path, e.size, e.modificationTime) for e in entries]\n",
    "display(spark.createDataFrame(rows, [\"path\", \"size\", \"mtime\"]).orderBy(\"path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2049cbf5-c338-4b64-aa02-ccb4cc09039f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round as rd\n",
    "\n",
    "fact = spark.read.format(\"delta\").load(f\"{abfss_url}gold/fact_rent\")\n",
    "dim_t = spark.read.format(\"delta\").load(f\"{abfss_url}gold/dim_time\")\n",
    "dim_s = spark.read.format(\"delta\").load(f\"{abfss_url}gold/dim_suburb\")\n",
    "\n",
    "# Trendline: average rent per month per region\n",
    "trend_region = (\n",
    "    fact.join(dim_t, \"time_id\")\n",
    "        .join(dim_s, \"suburb_id\")\n",
    "        .groupBy(\"region\", \"date_month\")\n",
    "        .agg(rd(avg(\"median_rent\"), 2).alias(\"avg_rent\"))\n",
    "        .orderBy(\"region\", \"date_month\")\n",
    ")\n",
    "\n",
    "display(trend_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1e9b80-7a4c-44a2-b5bc-37bc5ee38e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn DBFS (Databricks FileStore path)\n",
    "save_dir = \"/dbfs/FileStore/reports/\"\n",
    "\n",
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(\"Directory ready:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fdb621-bcfb-443e-9573-72d0736067cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame -> Pandas\n",
    "pd_trend = trend_region.toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "for region, g in pd_trend.groupby(\"region\"):\n",
    "    plt.plot(g[\"date_month\"], g[\"avg_rent\"], marker=\"o\", label=region)\n",
    "\n",
    "plt.title(\"Average Median Rent by Region (NZ)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Average Rent (NZD)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/dbfs/FileStore/reports/trendline_region.png\", dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fe2fd3-e97a-4676-8968-7a2953791e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, round as rd, col\n",
    "\n",
    "w = Window.partitionBy(\"suburb_id\").orderBy(\"date_month\")\n",
    "\n",
    "df_mom = (\n",
    "    fact.join(dim_t, \"time_id\")\n",
    "        .join(dim_s, \"suburb_id\")\n",
    "        .withColumn(\"prev_rent\", lag(\"median_rent\").over(w))\n",
    "        .withColumn(\"mom_pct\", rd((col(\"median_rent\") - col(\"prev_rent\")) / col(\"prev_rent\") * 100, 2))\n",
    "        .filter(col(\"mom_pct\").isNotNull())\n",
    ")\n",
    "\n",
    "latest_month = df_mom.agg({\"date_month\": \"max\"}).collect()[0][0]\n",
    "top_gainers = (\n",
    "    df_mom.filter(col(\"date_month\") == latest_month)\n",
    "           .orderBy(col(\"mom_pct\").desc())\n",
    "           .limit(10)\n",
    ")\n",
    "\n",
    "display(top_gainers.select(\"region\", \"suburb_name\", \"mom_pct\", \"median_rent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50725f7-fed0-40b9-9b05-77c77887f306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_top = top_gainers.toPandas()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.barh(pd_top[\"suburb_name\"], pd_top[\"mom_pct\"], color=\"skyblue\")\n",
    "plt.title(\"Top Month-over-Month Gainers – Latest Month\")\n",
    "plt.xlabel(\"Change vs Previous Month (%)\")\n",
    "plt.ylabel(\"Suburb\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/dbfs/FileStore/reports/top_mom_gainers.png\", dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3385971d-e8c6-407c-b739-dca27110dc58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev_samp\n",
    "\n",
    "w3 = Window.partitionBy(\"suburb_id\").orderBy(\"date_month\").rowsBetween(-2, 0)\n",
    "\n",
    "df_vol = (\n",
    "    fact.join(dim_t, \"time_id\")\n",
    "        .join(dim_s, \"suburb_id\")\n",
    "        .withColumn(\"rolling3_std\", rd(stddev_samp(\"median_rent\").over(w3), 2))\n",
    ")\n",
    "\n",
    "latest_month = df_vol.agg({\"date_month\": \"max\"}).collect()[0][0]\n",
    "top_vol = (\n",
    "    df_vol.filter(col(\"date_month\") == latest_month)\n",
    "           .orderBy(col(\"rolling3_std\").desc())\n",
    "           .limit(10)\n",
    ")\n",
    "\n",
    "display(top_vol.select(\"region\", \"suburb_name\", \"rolling3_std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "956d58a1-05ef-447c-bb02-dca9994207e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_vol = top_vol.toPandas()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(pd_vol[\"suburb_name\"], pd_vol[\"rolling3_std\"], s=80, alpha=0.7)\n",
    "plt.title(\"Top Volatile Suburbs (3-Month Rolling STD)\")\n",
    "plt.xlabel(\"Suburb\")\n",
    "plt.ylabel(\"Rolling 3-Month Std (NZD)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/dbfs/FileStore/reports/volatility.png\", dpi=120)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_analytics_queries",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
