{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91da940-8554-4783-a97c-e4ac0f8982f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ADLS via SAS (FixedSASTokenProvider) â€” short & robust -----------------\n",
    "storage_account = \"stnzrentdev\"\n",
    "container = \"nz-rent\"\n",
    "dfs_fqdn = f\"{storage_account}.dfs.core.windows.net\"\n",
    "abfss_url = f\"abfss://{container}@{dfs_fqdn}/\"\n",
    "\n",
    "# Use token WITHOUT leading '?'\n",
    "sas_token_raw = \"sv=2024-11-04&ss=bfqt&srt=co&sp=rwdlacupyx&se=2025-10-25T16:26:07Z&st=2025-10-15T08:11:07Z&spr=https&sig=Xddwgamve%2Fr6c2FKAWLKWax2cOWBZwUJ5t%2BpmxPWOdg%3D\"\n",
    "\n",
    "# 0) Clear possible conflicting configs (ignore errors if not set)\n",
    "for k in [\n",
    "    f\"fs.azure.account.key.{dfs_fqdn}\",\n",
    "    f\"fs.azure.sas.{container}.{storage_account}.dfs.core.windows.net\",\n",
    "]:\n",
    "    try: spark.conf.unset(k)\n",
    "    except Exception: pass\n",
    "\n",
    "# 1) Tell Spark to use SAS with FixedSASTokenProvider\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{dfs_fqdn}\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{dfs_fqdn}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{dfs_fqdn}\", sas_token_raw)\n",
    "\n",
    "# 2) List container; bootstrap /bronze if empty\n",
    "entries = dbutils.fs.ls(abfss_url)\n",
    "if len(entries) == 0:\n",
    "    dbutils.fs.mkdirs(abfss_url + \"bronze\")\n",
    "    dbutils.fs.put(abfss_url + \"bronze/_sanity.txt\", \"hello databricks\", overwrite=True)\n",
    "    entries = dbutils.fs.ls(abfss_url)\n",
    "\n",
    "# 3) Show as table\n",
    "rows = [(e.path, e.size, e.modificationTime) for e in entries]\n",
    "display(spark.createDataFrame(rows, [\"path\", \"size\", \"mtime\"]).orderBy(\"path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d130e18c-842e-4d5f-8c97-78d5523ac137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, year, month, quarter, when, round as rd\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "bronze_path = f\"{abfss_url}bronze/staging_rent_from_workspace.csv\"\n",
    "\n",
    "df_bronze = (spark.read\n",
    "             .option(\"header\", True)\n",
    "             .option(\"inferSchema\", True)\n",
    "             .csv(bronze_path))\n",
    "\n",
    "df_silver = (df_bronze\n",
    "    .withColumn(\"suburb_name\", trim(col(\"suburb_name\")))\n",
    "    .withColumn(\"region\", when(trim(col(\"region\")) == \"\", None).otherwise(trim(col(\"region\"))))\n",
    "    .withColumn(\"territorial_authority\", when(trim(col(\"territorial_authority\")) == \"\", None).otherwise(trim(col(\"territorial_authority\"))))\n",
    "    .withColumn(\"property_type\", trim(col(\"property_type\")))\n",
    "\n",
    "    .withColumn(\"median_rent\", col(\"median_rent\").cast(\"double\"))\n",
    "    .withColumn(\"count_bonds\", col(\"count_bonds\").cast(\"int\"))\n",
    "    \n",
    "    .withColumn(\"date_month\", col(\"date_month\").cast(\"date\"))\n",
    "    .withColumn(\"year\", year(col(\"date_month\")))\n",
    "    .withColumn(\"month\", month(col(\"date_month\")))\n",
    "    .withColumn(\"quarter\", quarter(col(\"date_month\")))\n",
    ")\n",
    "    \n",
    "df_silver = df_silver.filter(col(\"date_month\").isNotNull() & col(\"median_rent\").isNotNull())\n",
    "\n",
    "silver_path = f\"{abfss_url}silver/staging_rent_delta\"\n",
    "(df_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(silver_path))\n",
    "\n",
    "print(\"Silver layer written successfully.\")\n",
    "display(df_silver.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95526c85-636e-42ac-930d-bb1e8fdbccb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "silver_path = f\"{abfss_url}silver/staging_rent_delta\"\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "dim_time = (df_silver\n",
    "            .select(\"date_month\", \"year\", \"quarter\", \"month\")\n",
    "            .distinct()\n",
    "            .orderBy(\"date_month\")\n",
    "            .withColumn(\"time_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "dim_time.write.format(\"delta\").mode(\"overwrite\").save(f\"{abfss_url}gold/dim_time\")\n",
    "\n",
    "dim_suburb = (df_silver\n",
    "              .select(\"suburb_name\", \"region\", \"territorial_authority\", \"lat\", \"lon\")\n",
    "              .distinct()\n",
    "              .withColumn(\"suburb_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "dim_suburb.write.format(\"delta\").mode(\"overwrite\").save(f\"{abfss_url}gold/dim_suburb\")\n",
    "\n",
    "dim_property_type = (df_silver\n",
    "                .select(trim(col(\"property_type\")).alias(\"property_type_name\"))\n",
    "                .distinct()\n",
    "                .withColumn(\"property_type_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "dim_property_type.write.format(\"delta\").mode(\"overwrite\").save(f\"{abfss_url}gold/dim_property_type\")\n",
    "\n",
    "print(\"Dimension tables created successfully.\")\n",
    "display(dim_suburb.limit(5))\n",
    "display(dim_property_type.limit(5))\n",
    "display(dim_time.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f18d984-1296-45c7-b3e3-9226d2362c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = f\"{abfss_url}silver/staging_rent_delta\"\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "dim_time = spark.read.format(\"delta\").load(f\"{abfss_url}gold/dim_time\")\n",
    "dim_suburb = spark.read.format(\"delta\").load(f\"{abfss_url}gold/dim_suburb\")\n",
    "dim_property = spark.read.format(\"delta\").load(f\"{abfss_url}gold/dim_property_type\")\n",
    "\n",
    "fact_rent = (\n",
    "    df_silver.alias(\"r\")\n",
    "    .join(dim_time.alias(\"t\"), col(\"r.date_month\") == col(\"t.date_month\"), \"inner\")\n",
    "    .join(\n",
    "        dim_suburb.alias(\"s\"),\n",
    "        (col(\"r.suburb_name\") == col(\"s.suburb_name\")) &\n",
    "        (col(\"r.region\") == col(\"s.region\")),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        dim_property.alias(\"p\"),\n",
    "        col(\"r.property_type\") == col(\"p.property_type_name\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"t.time_id\"),\n",
    "        col(\"s.suburb_id\"),\n",
    "        col(\"p.property_type_id\"),\n",
    "        col(\"r.median_rent\").alias(\"median_rent\"),\n",
    "        col(\"r.count_bonds\").alias(\"count_bonds\")\n",
    "    )\n",
    ")\n",
    "\n",
    "fact_path = f\"{abfss_url}gold/fact_rent\"\n",
    "(fact_rent.write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(fact_path))\n",
    "\n",
    "print(\"Fact table created successfully.\")\n",
    "display(fact_rent.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_silver_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
