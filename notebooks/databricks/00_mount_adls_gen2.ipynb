{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcfd675-f7ec-4f85-a682-398b9d9fd351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ADLS via SAS (FixedSASTokenProvider) â€” short & robust -----------------\n",
    "storage_account = \"stnzrentdev\"\n",
    "container = \"nz-rent\"\n",
    "dfs_fqdn = f\"{storage_account}.dfs.core.windows.net\"\n",
    "abfss_url = f\"abfss://{container}@{dfs_fqdn}/\"\n",
    "\n",
    "# Use token WITHOUT leading '?'\n",
    "sas_token_raw = \"sv=2024-11-04&ss=bfqt&srt=co&sp=rwdlacupyx&se=2025-10-25T16:26:07Z&st=2025-10-15T08:11:07Z&spr=https&sig=Xddwgamve%2Fr6c2FKAWLKWax2cOWBZwUJ5t%2BpmxPWOdg%3D\"\n",
    "\n",
    "# 0) Clear possible conflicting configs (ignore errors if not set)\n",
    "for k in [\n",
    "    f\"fs.azure.account.key.{dfs_fqdn}\",\n",
    "    f\"fs.azure.sas.{container}.{storage_account}.dfs.core.windows.net\",\n",
    "]:\n",
    "    try: spark.conf.unset(k)\n",
    "    except Exception: pass\n",
    "\n",
    "# 1) Tell Spark to use SAS with FixedSASTokenProvider\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{dfs_fqdn}\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{dfs_fqdn}\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{dfs_fqdn}\", sas_token_raw)\n",
    "\n",
    "# 2) List container; bootstrap /bronze if empty\n",
    "entries = dbutils.fs.ls(abfss_url)\n",
    "if len(entries) == 0:\n",
    "    dbutils.fs.mkdirs(abfss_url + \"bronze\")\n",
    "    dbutils.fs.put(abfss_url + \"bronze/_sanity.txt\", \"hello databricks\", overwrite=True)\n",
    "    entries = dbutils.fs.ls(abfss_url)\n",
    "\n",
    "# 3) Show as table\n",
    "rows = [(e.path, e.size, e.modificationTime) for e in entries]\n",
    "display(spark.createDataFrame(rows, [\"path\", \"size\", \"mtime\"]).orderBy(\"path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2241e66-82b4-473e-8c07-9a5015720aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "local_src = \"/FileStore/shared_uploads/zym0170@autuni.ac.nz/staging_rent.csv\"\n",
    "bronze_dst = f\"{abfss_url}bronze/staging_rent_from_workspace.csv\"\n",
    "\n",
    "dbutils.fs.cp(local_src, bronze_dst, recurse=False)\n",
    "display(dbutils.fs.ls(f\"{abfss_url}bronze/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa9af55-723d-407a-a1df-bcb1d542322a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "bronze_csv = f\"{abfss_url}bronze/staging_rent_from_workspace.csv\"\n",
    "\n",
    "df_bronze = (spark.read\n",
    "             .option(\"header\", True)\n",
    "             .option(\"inferSchema\", True)\n",
    "             .csv(bronze_csv))\n",
    "\n",
    "print(\"Bronze rows:\", df_bronze.count())\n",
    "df_bronze.printSchema()\n",
    "display(df_bronze.limit(10))\n",
    "\n",
    "display(df_bronze.select('region').distinct().orderBy('region'))\n",
    "display(df_bronze.select('property_type').distinct().orderBy('property_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29699942-713b-423b-9680-83b836f5be80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_delta = f\"{abfss_url}bronze_delta/staging_rent_delta\"\n",
    "(df_bronze.write\n",
    " .format('delta')\n",
    " .mode('overwrite')\n",
    " .save(bronze_delta))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_mount_adls_gen2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
